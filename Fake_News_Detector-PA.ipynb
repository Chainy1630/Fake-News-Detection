{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1374cebd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Fake News Detector - Training\n",
    "# ===============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# -------------------------------\n",
    "# Download NLTK resources\n",
    "# -------------------------------\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# -------------------------------\n",
    "# Load dataset\n",
    "# -------------------------------\n",
    "train_df = pd.read_csv(r'/miniproject/dataset/train.csv')\n",
    "print(\"Dataset loaded:\", train_df.shape)\n",
    "print(train_df.head())\n",
    "\n",
    "# Drop unused columns\n",
    "train_df = train_df.drop([\"author\", \"title\", \"id\"], axis=1)\n",
    "\n",
    "# -------------------------------\n",
    "# Check distribution\n",
    "# -------------------------------\n",
    "def create_distribution(dataFile):\n",
    "    return sb.countplot(x='label', data=dataFile, palette='hls')\n",
    "\n",
    "create_distribution(train_df)\n",
    "\n",
    "# -------------------------------\n",
    "# Check for missing values\n",
    "# -------------------------------\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "# -------------------------------\n",
    "# Preprocess text (clean + lemmatize + remove stopwords)\n",
    "# -------------------------------\n",
    "def preprocess_text(text):\n",
    "    review = re.sub(r'[^a-zA-Z\\s]', '', str(text))\n",
    "    review = review.lower()\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    cleaned = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(cleaned)\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
    "\n",
    "# -------------------------------\n",
    "# Split dataset\n",
    "# -------------------------------\n",
    "X = train_df['text']\n",
    "y = train_df['label']   # <-- make sure 'label' column still exists\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# -------------------------------\n",
    "# TF-IDF Vectorizer\n",
    "# -------------------------------\n",
    "tfidf_v = TfidfVectorizer(max_df=0.7, stop_words='english')\n",
    "tfidf_X_train = tfidf_v.fit_transform(X_train)\n",
    "tfidf_X_test = tfidf_v.transform(X_test)\n",
    "\n",
    "# -------------------------------\n",
    "# Train model\n",
    "# -------------------------------\n",
    "classifier = PassiveAggressiveClassifier(max_iter=50)\n",
    "classifier.fit(tfidf_X_train, y_train)\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluate model\n",
    "# -------------------------------\n",
    "y_pred = classifier.predict(tfidf_X_test)\n",
    "score = metrics.accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {round(score*100, 2)}%\")\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix'):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "plot_confusion_matrix(cm, classes=['REAL', 'FAKE'])\n",
    "plt.show()\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=['REAL', 'FAKE']))\n",
    "\n",
    "# -------------------------------\n",
    "# Save model and vectorizer\n",
    "# -------------------------------\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(classifier, f)\n",
    "\n",
    "with open(\"vector.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tfidf_v, f)\n",
    "\n",
    "print(\"Model and vectorizer saved successfully!\")\n",
    "\n",
    "# -------------------------------\n",
    "# Quick test function\n",
    "# -------------------------------\n",
    "def fake_news_det(news):\n",
    "    cleaned = preprocess_text(news)\n",
    "    input_data = [cleaned]\n",
    "    vectorized_input = tfidf_v.transform(input_data)\n",
    "    prediction = classifier.predict(vectorized_input)\n",
    "\n",
    "    if prediction[0] == 1:\n",
    "        print(\"âš  Fake News ðŸ“°\")\n",
    "    else:\n",
    "        print(\"âœ… Real News ðŸ“°\")\n",
    "\n",
    "# -------------------------------\n",
    "# Try manual test\n",
    "# -------------------------------\n",
    "news = \"India successfully launched the Aditya-L1 solar mission from Sriharikota.\"\n",
    "fake_news_det(news)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
